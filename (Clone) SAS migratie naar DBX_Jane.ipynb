{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ecd54ec-5a69-41c3-9057-9d03f7e5bd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extract Data and Metadata from XML\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795d4ec4-3481-40e8-99b0-268e9af18f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Element> and root entries\n",
    "# root is ProjectCollection\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, BooleanType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# in _config komen losse entries in de root \n",
    "parsed_data = []\n",
    "d = {\"Element\": {}, \"_config\": {}, \"MetaDataInfo\": {}, \"DecisionManager\":{}, \"Parameters\": {}, \"Containers\": {}}\n",
    "for branch in root:\n",
    "    if branch.tag == \"Element\":\n",
    "        for child in branch:\n",
    "            d[\"Element\"][child.tag] = child.text\n",
    "    elif branch.tag == \"UseRelativePaths\":\n",
    "        d[\"_config\"][branch.tag] = branch.text\n",
    "    elif branch.tag == \"SubmitToGrid\":\n",
    "        d[\"_config\"][branch.tag] = branch.text\n",
    "    elif branch.tag == \"QueueSubmitsForServer\":\n",
    "        d[\"_config\"][branch.tag] = branch.text\n",
    "    elif branch.tag == \"ActionOnError\":\n",
    "        d[\"_config\"][branch.tag] = branch.text\n",
    "    elif branch.tag == \"ApplicationOverrides\":\n",
    "        d[\"_config\"][\"ApplicationOverrides\"] = branch.text\n",
    "    elif branch.tag == \"ExploreDataList\":\n",
    "        d[\"_config\"][\"ExploreDataList\"] = branch.text\n",
    "    elif branch.tag == \"MetaDataInfo\":\n",
    "        for child in branch:\n",
    "            d[\"MetaDataInfo\"][child.tag] = child.text\n",
    "    elif branch.tag == \"DecisionManager\":\n",
    "        for child in branch:\n",
    "            d[\"DecisionManager\"][child.tag] = child.text\n",
    "    elif branch.tag == \"Parameters\":\n",
    "        for child in branch:\n",
    "            d[\"Parameters\"][child.tag] = child.text\n",
    "    elif branch.tag == \"Containers\":\n",
    "        for child in branch:\n",
    "            d[\"Containers\"][child.tag] = child.text\n",
    "parsed_data.append(d)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Element\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"_config\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"MetaDataInfo\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"DecisionManager\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"Parameters\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"Containers\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "rootDF = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "display(rootDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822b589c-ebbf-47f2-a95a-73cac77c0a3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# root is ProjectCollection\n",
    "# branch is DataList\n",
    "# Data is data\n",
    "# element and data are the child van de data\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(data):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file datalist\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in data:\n",
    "        if child.tag == \"Element\":\n",
    "            d[child.tag] = parse_deepest(child)\n",
    "        if child.tag == \"Data\":\n",
    "            for el in child:\n",
    "                if el.tag == \"ShortCutList\":\n",
    "                    d[el.tag] = parse_deepest(el)\n",
    "                if el.tag == \"DataModel\":\n",
    "                    d[el.tag] = parse_deepest(el)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"DataList\":\n",
    "        for data in branch:\n",
    "            d = parse_data(data)\n",
    "            #print(d)\n",
    "            parsed_data.append(d)\n",
    "\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "datalistDF = spark.createDataFrame(rows)\n",
    "\n",
    "display(datalistDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2affce6d-d466-42b0-9899-24a0cc7d2b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <ExternalFileList>\n",
    "# root is ProjectCollection\n",
    "# branch is ExternalFileList\n",
    "# ExtrenalFile is externalfile\n",
    "# element and externalfile are the child van de externalfile\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(externalfile):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file externalfilelist\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in externalfile:\n",
    "        if child.tag == \"Element\":\n",
    "            d[child.tag] = parse_deepest(child)\n",
    "        if child.tag == \"ExternalFile\":\n",
    "            for el in child:\n",
    "                if el.tag == \"ShortCutList\":\n",
    "                    d[el.tag] = parse_deepest(el)\n",
    "                else:\n",
    "                    d[el.tag] = el.text\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"ExternalFileList\":\n",
    "        for externalfile in branch:\n",
    "            d = parse_data(externalfile)\n",
    "            #print(d)\n",
    "            parsed_data.append(d)\n",
    "\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "externalFileListDF = spark.createDataFrame(rows)\n",
    "\n",
    "display(externalFileListDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdae11f0-8d31-4ccd-ba08-a1d484ab07cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Elements> of type PFD\n",
    "# root is ProjectCollection\n",
    "# branch is Elements\n",
    "# Element is element with attribute type\n",
    "# element, containerelement and pfd are the child van de element\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(outerelement):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file Elements\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in outerelement:\n",
    "        if child.tag == \"PFD\":\n",
    "            d[child.tag] = []\n",
    "            for el in child:\n",
    "                if el.tag == \"Process\":\n",
    "                    d2 = {}\n",
    "                    for var in el:\n",
    "                        if var.tag == \"Element\":\n",
    "                            d2[var.tag] = parse_deepest(var)\n",
    "                        if var.tag == \"Depencies\":\n",
    "                            d2[var.tag] = parse_deepest(var)\n",
    "                    d[child.tag].append(d2)\n",
    "        else:\n",
    "            d[child.tag] = parse_deepest(child)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"Elements\":\n",
    "        for outerelement in branch:\n",
    "            if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.PFD\":\n",
    "                d = parse_data(outerelement)\n",
    "                d.setdefault(\"Element\", {})\n",
    "                d.setdefault(\"ContainerElement\", {})\n",
    "                d.setdefault(\"PFD\", [])\n",
    "                #print(d)\n",
    "                parsed_data.append(d)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Element\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"ContainerElement\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"PFD\", ArrayType(MapType(StringType(), StringType())), True)\n",
    "])\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "PFDelementsDF = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "display(PFDelementsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604511c1-9cea-44ea-818a-e7d3445744d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Elements> of type ShortCutToData\n",
    "# root is ProjectCollection\n",
    "# branch is Elements\n",
    "# Element is element with attribute type\n",
    "# element, containerelement and pfd are the child van de element\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(outerelement):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file Elements\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in outerelement:\n",
    "        d[child.tag] = parse_deepest(child)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"Elements\":\n",
    "        for outerelement in branch:\n",
    "            # if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.ShortCutToFile\":\n",
    "            if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.ShortCutToData\":\n",
    "                d = parse_data(outerelement)\n",
    "                d.setdefault(\"Element\", {})\n",
    "                #print(d)\n",
    "                parsed_data.append(d)\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "SCTDelementsDF = spark.createDataFrame(rows)\n",
    "\n",
    "display(SCTDelementsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb224f0-177a-4249-88dd-2e2932b1ce7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Elements> of type ShortCutToFile\n",
    "# root is ProjectCollection\n",
    "# branch is Elements\n",
    "# Element is element with attribute type\n",
    "# element, containerelement and pfd are the child van de element\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(outerelement):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file Elements\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in outerelement:\n",
    "        d[child.tag] = parse_deepest(child) if child is not None else {}\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"Elements\":\n",
    "        for outerelement in branch:\n",
    "            if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.ShortCutToFile\":\n",
    "                d = parse_data(outerelement)\n",
    "                d.setdefault(\"Element\", {})\n",
    "                d.setdefault(\"SHORTCUT\", {})\n",
    "                d.setdefault(\"ShortCutToFile\", {})\n",
    "                #print(d)\n",
    "                parsed_data.append(d)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Element\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"SHORTCUT\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"ShortCutToFile\", MapType(StringType(), StringType()), True),\n",
    "])\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "SCTFelementsDF = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "display(SCTFelementsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7283c7d8-bc9c-4f20-8766-c0a64caaea23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Elements> of type Log\n",
    "# root is ProjectCollection\n",
    "# branch is Elements\n",
    "# Element is element with attribute type\n",
    "# element, containerelement and pfd are the child van de element\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(outerelement):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file Elements\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in outerelement:\n",
    "        d[child.tag] = parse_deepest(child)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"Elements\":\n",
    "        for outerelement in branch:\n",
    "            if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.Log\":\n",
    "                d = parse_data(outerelement)\n",
    "                d.setdefault(\"Element\", {})\n",
    "                #print(d)\n",
    "                parsed_data.append(d)\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "LOGelementsDF = spark.createDataFrame(rows)\n",
    "\n",
    "display(LOGelementsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea8a467-58a2-4e25-8ae6-d80faaa7b868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Elements> of type Code\n",
    "# root is ProjectCollection\n",
    "# branch is Elements\n",
    "# Element is element with attribute type\n",
    "# element, containerelement and pfd are the child van de element\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(outerelement):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file Elements\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in outerelement:\n",
    "        d[child.tag] = parse_deepest(child)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"Elements\":\n",
    "        for outerelement in branch:\n",
    "            if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.Code\":\n",
    "                d = parse_data(outerelement)\n",
    "                d.setdefault(\"Element\", {})\n",
    "                #print(d)\n",
    "                parsed_data.append(d)\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "CODEelementsDF = spark.createDataFrame(rows)\n",
    "\n",
    "display(CODEelementsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40240cf6-9b3f-4567-983a-2921dc642c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Elements> of type ImportTask\n",
    "# root is ProjectCollection\n",
    "# branch is Elements\n",
    "# Element is element with attribute type\n",
    "# element, containerelement and pfd are the child van de element\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_deepest(var, prefix=\"\"):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        if prefix == \"\" and child.tag == \"OutputDataList\":\n",
    "            parse_deepest(child, \"OutputDataList_\")\n",
    "        else:\n",
    "            d[prefix + child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(data):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file datalist\n",
    "    \"\"\"\n",
    "    d = {\n",
    "        \"Element\": {},\n",
    "        \"SubmitableElement\": {},\n",
    "        \"ExpectedOutputDataList\": {},\n",
    "        \"JobRecipe\": {},\n",
    "        \"EGTask\": {},\n",
    "        \"ImportTask\": []\n",
    "    }\n",
    "    \n",
    "    for child in data:\n",
    "        if child.tag == \"SubmitableElement\":\n",
    "            for el in child:\n",
    "                if el.tag == \"ExpectedOutputDataList\":\n",
    "                    for var in el:\n",
    "                        if var.tag == \"DataDescriptor\":\n",
    "                            d[\"ExpectedOutputDataList\"] = parse_deepest(var, \"DataDescriptor_\")\n",
    "                        else:\n",
    "                            d[\"ExpectedOutputDataList\"][var.tag] = var.text\n",
    "                if el.tag == \"JobRecipe\":\n",
    "                    # Jobrecipe includes one element also called jobrecipe\n",
    "                    # iterate over the inner jobrecipe \n",
    "                    for var in el[0]:\n",
    "                        print(var)\n",
    "                        if var.tag == \"OutputDataList\":\n",
    "                            d[\"JobRecipe\"] = parse_deepest(var, \"OutputDataList_\")\n",
    "                        else:\n",
    "                            d[\"JobRecipe\"][var.tag] = var.text\n",
    "                else:\n",
    "                    d[\"SubmitableElement\"][el.tag] = el.text\n",
    "        elif child.tag == \"ImportTask\":\n",
    "            d[\"ImportTask\"].append(parse_deepest(child))\n",
    "        else:\n",
    "            d[child.tag] = parse_deepest(child)\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"Elements\":\n",
    "        for outerelement in branch:\n",
    "            if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.ImportTask\":\n",
    "                d = parse_data(outerelement)\n",
    "                d.setdefault(\"Element\", {})\n",
    "                #print(d)\n",
    "                parsed_data.append(d)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Element\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"SubmitableElement\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"ExpectedOutputDataList\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"JobRecipe\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"EGTask\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"ImportTask\", ArrayType(StringType()), True)\n",
    "])\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "IMPTelementsDF = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "display(IMPTelementsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9234c3c0-9284-4844-b053-341c94061e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parse <Elements> of type ShortCutToFile\n",
    "# root is ProjectCollection\n",
    "# branch is Elements\n",
    "# Element is element with attribute type\n",
    "# element, containerelement and pfd are the child van de element\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType\n",
    "from pyspark.sql import Row\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_deepest(var):\n",
    "    d = {}\n",
    "    for child in var:\n",
    "        d[child.tag] = child.text\n",
    "    return d\n",
    "\n",
    "def parse_data(outerelement):\n",
    "    \"\"\"\n",
    "    Parses a SAS XML file Elements\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    for child in outerelement:\n",
    "        d[child.tag] = parse_deepest(child) if child is not None else {}\n",
    "    return d\n",
    "\n",
    "\n",
    "# Load and parse the XML\n",
    "xml_path = \"/Volumes/janesun/default/sas_xml/project_1.xml\"\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "parsed_data = []\n",
    "for branch in root:\n",
    "    if branch.tag == \"Elements\":\n",
    "        for outerelement in branch:\n",
    "            if outerelement.attrib[\"Type\"] == \"SAS.EG.ProjectElements.ShortCutToFile\":\n",
    "                d = parse_data(outerelement)\n",
    "                d.setdefault(\"Element\", {})\n",
    "                d.setdefault(\"SHORTCUT\", {})\n",
    "                d.setdefault(\"ShortCutToFile\", {})\n",
    "                #print(d)\n",
    "                parsed_data.append(d)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Element\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"SHORTCUT\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"ShortCutToFile\", MapType(StringType(), StringType()), True),\n",
    "])\n",
    "\n",
    "# Convert parsed data to Spark DataFrame\n",
    "rows = [Row(**item) for item in parsed_data]\n",
    "SCTFelementsDF = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "display(SCTFelementsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "443fbb7c-0c98-49d1-9527-59d204ea5fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5d66c10-dcd9-46cb-8f38-1efe7a446898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Drop table if exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5cd8f09-d3d8-43a3-a93a-926ab1591c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Define library_name and table_name\n",
    "#library_name = \"/janesun/sas_data\"  # Replace with your actual library name\n",
    "datalist_table_name = \"JG_Start_C7_Managing_DataList\"  # Replace with your actual table name\n",
    "external_file_list_table_name = \"JG_Start_C7_Managing_externalFileList\"\n",
    "elements_table_name = \"JG_Start_C7_Managing_elements\"\n",
    "\n",
    "# Create database and table\n",
    "catalog_name = \"janesun\"\n",
    "database_name = \"sas_data\"\n",
    "library_name_fixed = f\"{catalog_name}.{database_name}\"\n",
    "\n",
    "# Create database and table\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {library_name_fixed}\")\n",
    "datalistDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{library_name_fixed}.{datalist_table_name.replace('/', '_').replace(' ', '_')}\")\n",
    "\n",
    "externalFileListDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{library_name_fixed}.{external_file_list_table_name.replace('/', '_').replace(' ', '_')}\")\n",
    "\n",
    "elementsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{library_name_fixed}.{elements_table_name.replace('/', '_').replace(' ', '_')}\")\n",
    "\n",
    "display(datalistDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aced031-fc65-4ae4-a505-1b35d27617af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Convert SAS Code to pyspark or spark SQL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387fc76b-1405-48c3-9647-cb4a198b7009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Migrate Data Workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d058520-1357-4bc4-b95e-36a73017e75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Migrate Analytics and Reporting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4320a2a9-4951-4027-8bf7-d76a6c696b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optimize performance\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8129779188827907,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) SAS migratie naar DBX_Jane",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
